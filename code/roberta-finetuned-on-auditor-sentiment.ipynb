{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    AutoConfig,\n)\nfrom huggingface_hub import HfFolder, notebook_login","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-18T15:41:39.417850Z","iopub.execute_input":"2024-08-18T15:41:39.418697Z","iopub.status.idle":"2024-08-18T15:41:46.372828Z","shell.execute_reply.started":"2024-08-18T15:41:39.418665Z","shell.execute_reply":"2024-08-18T15:41:46.371802Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-18 15:41:43.065946: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-18 15:41:43.066009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-18 15:41:43.067615: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:41:50.042408Z","iopub.execute_input":"2024-08-18T15:41:50.043418Z","iopub.status.idle":"2024-08-18T15:41:50.065308Z","shell.execute_reply.started":"2024-08-18T15:41:50.043370Z","shell.execute_reply":"2024-08-18T15:41:50.064429Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f750ab489f1d40c8937834feb7a53904"}},"metadata":{}}]},{"cell_type":"code","source":"model_id = \"roberta-base\"\ndataset_id = \"FinanceInc/auditor_sentiment\"\n# relace the value with your model: ex <hugging-face-user>/<model-name>\nrepository_id = \"qwdf8591/roberta-base_auditor_sentiment\"","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:04.603128Z","iopub.execute_input":"2024-08-18T15:42:04.603552Z","iopub.status.idle":"2024-08-18T15:42:04.608287Z","shell.execute_reply.started":"2024-08-18T15:42:04.603518Z","shell.execute_reply":"2024-08-18T15:42:04.607261Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(dataset_id)\n\n# Training and testing datasets\ntrain_dataset = dataset['train']\ntest_dataset = dataset[\"test\"].shard(num_shards=2, index=0)\n\n# Validation dataset\nval_dataset = dataset['test'].shard(num_shards=2, index=1)\n\n# Preprocessing\ntokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n\n# This function tokenizes the input text using the RoBERTa tokenizer. \n# It applies padding and truncation to ensure that all sequences have the same length (256 tokens).\ndef tokenize(batch):\n    return tokenizer(batch[\"sentence\"], padding=True, truncation=True, max_length=512)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\nval_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\ntest_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:08.225840Z","iopub.execute_input":"2024-08-18T15:42:08.226494Z","iopub.status.idle":"2024-08-18T15:42:10.750732Z","shell.execute_reply.started":"2024-08-18T15:42:08.226452Z","shell.execute_reply":"2024-08-18T15:42:10.749825Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/485 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a72ea24de9451fba3363cc0a124a5f"}},"metadata":{}}]},{"cell_type":"code","source":"# Set dataset format\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:14.090627Z","iopub.execute_input":"2024-08-18T15:42:14.091347Z","iopub.status.idle":"2024-08-18T15:42:14.098580Z","shell.execute_reply.started":"2024-08-18T15:42:14.091305Z","shell.execute_reply":"2024-08-18T15:42:14.097579Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:15.801525Z","iopub.execute_input":"2024-08-18T15:42:15.802236Z","iopub.status.idle":"2024-08-18T15:42:15.808826Z","shell.execute_reply.started":"2024-08-18T15:42:15.802204Z","shell.execute_reply":"2024-08-18T15:42:15.807903Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'sentence': \"Altia 's operating profit jumped to EUR 47 million from EUR 6.6 million .\",\n 'label': 2}"},"metadata":{}}]},{"cell_type":"code","source":"num_labels = 3\nclass_names = [\"negative\", \"neutral\", \"positive\"]\n\nprint(f\"Number of labels: {num_labels}\")\nprint(f\"The labels: {class_names}\")\n# create id2label mapping\nid2label = {i: label for i, label in enumerate(class_names)}\nlabel2id = {label: i for i, label in enumerate(class_names)}\n\n# 更新模型的配置\nconfig = AutoConfig.from_pretrained(model_id)\nconfig.num_labels = num_labels\nconfig.id2label = id2label\nconfig.label2id = label2id\n\n# 打印配置以確認更新\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:17.937224Z","iopub.execute_input":"2024-08-18T15:42:17.938277Z","iopub.status.idle":"2024-08-18T15:42:18.048769Z","shell.execute_reply.started":"2024-08-18T15:42:17.938239Z","shell.execute_reply":"2024-08-18T15:42:18.047893Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of labels: 3\nThe labels: ['negative', 'neutral', 'positive']\nRobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"negative\",\n    \"1\": \"neutral\",\n    \"2\": \"positive\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"negative\": 0,\n    \"neutral\": 1,\n    \"positive\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.42.3\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\nprecision = evaluate.load(\"precision\")\nrecall = evaluate.load(\"recall\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    results = {\n        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n        \"precision\": precision.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"],\n        \"recall\": recall.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"],\n        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"],\n    }\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:25.345358Z","iopub.execute_input":"2024-08-18T15:42:25.346314Z","iopub.status.idle":"2024-08-18T15:42:27.741154Z","shell.execute_reply.started":"2024-08-18T15:42:25.346278Z","shell.execute_reply":"2024-08-18T15:42:27.740357Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Model\nmodel = RobertaForSequenceClassification.from_pretrained(model_id, config=config)\n\n# TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=repository_id,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_strategy=\"epoch\",\n    logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=2,\n    report_to=\"tensorboard\",\n    push_to_hub=True,\n    hub_strategy=\"every_save\",\n    hub_model_id=repository_id,\n    hub_token=HfFolder.get_token(),\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    compute_metrics=compute_metrics,\n    eval_dataset=val_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:40.758298Z","iopub.execute_input":"2024-08-18T15:42:40.759015Z","iopub.status.idle":"2024-08-18T15:42:41.297653Z","shell.execute_reply.started":"2024-08-18T15:42:40.758982Z","shell.execute_reply":"2024-08-18T15:42:41.296763Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:42:43.678774Z","iopub.execute_input":"2024-08-18T15:42:43.679151Z","iopub.status.idle":"2024-08-18T15:48:45.765369Z","shell.execute_reply.started":"2024-08-18T15:42:43.679123Z","shell.execute_reply":"2024-08-18T15:48:45.764218Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2425' max='2425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2425/2425 05:47, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.421700</td>\n      <td>0.635760</td>\n      <td>0.822314</td>\n      <td>0.814171</td>\n      <td>0.813525</td>\n      <td>0.813436</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.653800</td>\n      <td>0.649072</td>\n      <td>0.838843</td>\n      <td>0.858391</td>\n      <td>0.802512</td>\n      <td>0.819240</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.396100</td>\n      <td>0.535569</td>\n      <td>0.855372</td>\n      <td>0.822397</td>\n      <td>0.872231</td>\n      <td>0.841444</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.112100</td>\n      <td>0.739307</td>\n      <td>0.851240</td>\n      <td>0.841409</td>\n      <td>0.847688</td>\n      <td>0.842825</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.019200</td>\n      <td>0.723283</td>\n      <td>0.869835</td>\n      <td>0.858104</td>\n      <td>0.874291</td>\n      <td>0.865719</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2425, training_loss=0.3738708070408284, metrics={'train_runtime': 348.2939, 'train_samples_per_second': 55.657, 'train_steps_per_second': 6.963, 'total_flos': 1324922517877770.0, 'train_loss': 0.3738708070408284, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:49:55.427648Z","iopub.execute_input":"2024-08-18T15:49:55.428060Z","iopub.status.idle":"2024-08-18T15:49:56.826507Z","shell.execute_reply.started":"2024-08-18T15:49:55.428031Z","shell.execute_reply":"2024-08-18T15:49:56.825472Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [61/61 00:01]\n    </div>\n    "},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.5355690121650696,\n 'eval_accuracy': 0.8553719008264463,\n 'eval_precision': 0.822396689042232,\n 'eval_recall': 0.8722306360011278,\n 'eval_f1': 0.8414437298443005,\n 'eval_runtime': 1.3876,\n 'eval_samples_per_second': 348.807,\n 'eval_steps_per_second': 43.961,\n 'epoch': 5.0}"},"metadata":{}}]},{"cell_type":"code","source":"# Save our tokenizer and create a model card\ntokenizer.save_pretrained(repository_id)\ntrainer.create_model_card()\n# Push the results to the hub\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:50:14.516037Z","iopub.execute_input":"2024-08-18T15:50:14.516449Z","iopub.status.idle":"2024-08-18T15:50:20.454649Z","shell.execute_reply.started":"2024-08-18T15:50:14.516417Z","shell.execute_reply":"2024-08-18T15:50:20.453622Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8793d2524bff4dbfbab9bdb1362af011"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1723995764.d2d87c5b52e4.195.0:   0%|          | 0.00/58.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a94a23c8b4c0470d919d9aa89a9e600e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1723996196.d2d87c5b52e4.195.1:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5e45b4dcac47bd8ed23b14dd5b49bd"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/qwdf8591/roberta-base_auditor_sentiment/commit/ab9e415ea9e5a7cd47234a916acabaf9377501ae', commit_message='End of training', commit_description='', oid='ab9e415ea9e5a7cd47234a916acabaf9377501ae', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# TEST MODEL\n\nfrom transformers import pipeline\n\nclassifier = pipeline('text-classification',repository_id, device=0)\n\ntext = \"hi, I hate my life.\"\nclassifier(text)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T15:56:10.208338Z","iopub.execute_input":"2024-08-18T15:56:10.208757Z","iopub.status.idle":"2024-08-18T15:56:10.585265Z","shell.execute_reply.started":"2024-08-18T15:56:10.208727Z","shell.execute_reply":"2024-08-18T15:56:10.584288Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[{'label': 'negative', 'score': 0.9809571504592896}]"},"metadata":{}}]}]}